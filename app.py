"""
Keepa Excelçµåˆãƒ„ãƒ¼ãƒ«

å®Ÿè¡Œæ–¹æ³•:
    streamlit run app.py

æ©Ÿèƒ½:
    - keepa-*.xlsxãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰
    - ASINåˆ—ã®è¿½åŠ ï¼ˆå…¨ã‚«ãƒ©ãƒ ãŒå³ã«ã‚·ãƒ•ãƒˆï¼‰
    - è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¸¦çµåˆ
    - CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
"""

import streamlit as st
import pandas as pd
import openpyxl
from datetime import datetime
import io

st.set_page_config(page_title="Keepa Excelçµåˆãƒ„ãƒ¼ãƒ«", page_icon="ğŸ“Š", layout="wide")

st.title("ğŸ“Š Keepa Excelçµåˆãƒ„ãƒ¼ãƒ«")
st.markdown("è¤‡æ•°ã® `keepa-*.xlsx` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ASINåˆ—ã‚’è¿½åŠ ã—ã¦1ã¤ã®CSVã«çµåˆã—ã¾ã™ã€‚")

# ===== ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ– =====
if 'merged_df' not in st.session_state:
    st.session_state.merged_df = None
if 'file_list' not in st.session_state:
    st.session_state.file_list = []

# ===== ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ =====
st.subheader("ğŸ“¤ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")

uploaded_files = st.file_uploader(
    "keepa-*.xlsx ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰",
    type=["xlsx"],
    accept_multiple_files=True,
    help="Keepaå½¢å¼ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"
)

if uploaded_files:
    file_info = []
    for f in uploaded_files:
        # ã‚·ãƒ¼ãƒˆåã‹ã‚‰ASINã‚’å–å¾—ï¼ˆNoteä»¥å¤–ã®æœ€åˆã®ã‚·ãƒ¼ãƒˆï¼‰
        try:
            sheet_names = pd.ExcelFile(f).sheet_names
            asin = next((s for s in sheet_names if s.lower() != "note"), "ä¸æ˜")
        except Exception:
            asin = "ä¸æ˜"

        size_mb = len(f.getvalue()) / (1024 * 1024)

        file_info.append({
            "ãƒ•ã‚¡ã‚¤ãƒ«å": f.name,
            "ASIN": asin,
            "ã‚µã‚¤ã‚º (MB)": f"{size_mb:.2f}",
            "ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ": f
        })

    st.session_state.file_list = file_info
    st.success(f"âœ… {len(uploaded_files)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")

# ===== ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆè¡¨ç¤º =====
if st.session_state.file_list:
    st.subheader(f"ğŸ“‹ æ¤œå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ ({len(st.session_state.file_list)} ä»¶)")

    # è¡¨ç¤ºç”¨ã«ãƒ‘ã‚¹ã‚’é™¤å¤–
    display_df = pd.DataFrame(st.session_state.file_list)
    display_columns = ["ãƒ•ã‚¡ã‚¤ãƒ«å", "ASIN", "ã‚µã‚¤ã‚º (MB)"]
    st.dataframe(display_df[display_columns], use_container_width=True)

    # ===== çµåˆå‡¦ç† =====
    st.divider()

    if st.button("ğŸ”— çµåˆå®Ÿè¡Œ", type="primary", use_container_width=True):
        all_data = []
        progress_bar = st.progress(0)
        status_text = st.empty()

        for idx, file_info in enumerate(st.session_state.file_list):
            try:
                # ã‚·ãƒ¼ãƒˆåã‹ã‚‰æ­£ç¢ºãªASINã‚’å–å¾—
                status_text.text(f"å‡¦ç†ä¸­: {file_info['ãƒ•ã‚¡ã‚¤ãƒ«å']}")

                # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰ã®ã¿ï¼‰
                wb = openpyxl.load_workbook(
                    io.BytesIO(file_info["ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ"].getvalue()),
                    data_only=True
                )

                # ASINã‚·ãƒ¼ãƒˆã‚’æ¢ã™ï¼ˆNoteã‚·ãƒ¼ãƒˆä»¥å¤–ã®æœ€åˆã®ã‚·ãƒ¼ãƒˆ = ASINåï¼‰
                asin = next((name for name in wb.sheetnames if name.lower() != "note"), None)

                if not asin:
                    st.warning(f"âš ï¸ {file_info['ãƒ•ã‚¡ã‚¤ãƒ«å']}: ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆNoteã‚·ãƒ¼ãƒˆä»¥å¤–ï¼‰")
                    continue

                # ã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿
                file_info["ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ"].seek(0)
                df = pd.read_excel(file_info["ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ"], sheet_name=asin)

                # Aåˆ—ã«ASINã‚’è¿½åŠ ï¼ˆæ—¢å­˜ã‚«ãƒ©ãƒ ã‚’å³ã«ã‚·ãƒ•ãƒˆï¼‰
                df.insert(0, "ASIN", asin)

                all_data.append(df)

            except Exception as e:
                st.error(f"âŒ {file_info['ãƒ•ã‚¡ã‚¤ãƒ«å']}: ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ - {str(e)}")
                continue

            # é€²æ—æ›´æ–°
            progress_bar.progress((idx + 1) / len(st.session_state.file_list))

        if all_data:
            # ç¸¦çµåˆï¼ˆsort=Falseã§BSRã‚«ãƒ©ãƒ ç­‰ã®å·®ç•°ã‚‚å…¨ã¦ä¿æŒï¼‰
            st.session_state.merged_df = pd.concat(all_data, ignore_index=True, sort=False)
            status_text.text("âœ… çµåˆå®Œäº†!")
            progress_bar.empty()
            st.success(f"ğŸ‰ çµåˆå®Œäº†: {len(all_data)} ãƒ•ã‚¡ã‚¤ãƒ« â†’ {len(st.session_state.merged_df)} è¡Œ")
        else:
            status_text.text("âŒ çµåˆã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            progress_bar.empty()

# ===== çµåˆçµæœè¡¨ç¤º =====
if st.session_state.merged_df is not None:
    st.divider()
    st.subheader("ğŸ“Š çµåˆçµæœ")

    # æ—¥ä»˜ã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèªã¨å‹å¤‰æ›
    date_column = None
    if "æ—¥ä»˜" in st.session_state.merged_df.columns:
        date_column = "æ—¥ä»˜"
        st.session_state.merged_df[date_column] = pd.to_datetime(
            st.session_state.merged_df[date_column], errors='coerce'
        )
    elif "Date" in st.session_state.merged_df.columns:
        date_column = "Date"
        st.session_state.merged_df[date_column] = pd.to_datetime(
            st.session_state.merged_df[date_column], errors='coerce'
        )

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ç·è¡Œæ•°", f"{len(st.session_state.merged_df):,}")
    with col2:
        unique_asins = st.session_state.merged_df["ASIN"].nunique()
        st.metric("ASINæ•°", unique_asins)
    with col3:
        st.metric("ã‚«ãƒ©ãƒ æ•°", len(st.session_state.merged_df.columns))

    # æ—¥ä»˜ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    filtered_df = st.session_state.merged_df.copy()

    if date_column and st.session_state.merged_df[date_column].notna().any():
        st.divider()
        st.subheader("ğŸ“… æ—¥ä»˜ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")

        min_date = st.session_state.merged_df[date_column].min().date()
        max_date = st.session_state.merged_df[date_column].max().date()

        col_date1, col_date2 = st.columns(2)
        with col_date1:
            start_date = st.date_input(
                "é–‹å§‹æ—¥",
                value=min_date,
                min_value=min_date,
                max_value=max_date,
                help="ã“ã®æ—¥ä»˜ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"
            )
        with col_date2:
            end_date = st.date_input(
                "çµ‚äº†æ—¥",
                value=max_date,
                min_value=min_date,
                max_value=max_date,
                help="ã“ã®æ—¥ä»˜ä»¥å‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"
            )

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
        if start_date <= end_date:
            mask = (
                (st.session_state.merged_df[date_column].dt.date >= start_date) &
                (st.session_state.merged_df[date_column].dt.date <= end_date)
            )
            filtered_df = st.session_state.merged_df[mask].copy()

            if len(filtered_df) < len(st.session_state.merged_df):
                st.info(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœ: {len(filtered_df):,} è¡Œ / {len(st.session_state.merged_df):,} è¡Œ")
        else:
            st.error("âš ï¸ é–‹å§‹æ—¥ã¯çµ‚äº†æ—¥ã‚ˆã‚Šå‰ã«è¨­å®šã—ã¦ãã ã•ã„")

    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    st.divider()
    st.markdown("**ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå…ˆé ­10è¡Œï¼‰**")
    st.dataframe(filtered_df.head(10), use_container_width=True)

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    st.divider()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"keepa_merged_{timestamp}.csv"

    csv_buffer = io.StringIO()
    filtered_df.to_csv(csv_buffer, index=False, encoding="utf-8-sig")
    csv_data = csv_buffer.getvalue()

    st.download_button(
        label="ğŸ’¾ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv_data,
        file_name=csv_filename,
        mime="text/csv",
        type="primary",
        use_container_width=True
    )

    st.info(f"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«å: `{csv_filename}` ({len(filtered_df):,} è¡Œ)")

# ===== ãƒ•ãƒƒã‚¿ãƒ¼ =====
st.divider()
st.caption("ğŸ“ Tips: è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€åº¦ã«é¸æŠã§ãã¾ã™ï¼ˆCtrl/Cmd + ã‚¯ãƒªãƒƒã‚¯ï¼‰ã€‚")
