"""
Keepa Excelçµåˆãƒ„ãƒ¼ãƒ«

å®Ÿè¡Œæ–¹æ³•:
    streamlit run app.py

æ©Ÿèƒ½:
    - keepa-*.xlsxãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰
    - ASINåˆ—ã®è¿½åŠ ï¼ˆå…¨ã‚«ãƒ©ãƒ ãŒå³ã«ã‚·ãƒ•ãƒˆï¼‰
    - è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¸¦çµåˆ
    - CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
"""

import streamlit as st
import pandas as pd
import openpyxl
from datetime import datetime, date
from dateutil.relativedelta import relativedelta
from pathlib import Path
import io

# ===== ã‚»ãƒ¼ãƒ«æƒ…å ±èª­ã¿è¾¼ã¿ =====
def load_sale_periods():
    """CSVã‹ã‚‰ã‚»ãƒ¼ãƒ«æƒ…å ±ã‚’èª­ã¿è¾¼ã¿"""
    csv_path = Path(__file__).parent / "sale_periods.csv"
    try:
        df = pd.read_csv(csv_path, encoding='utf-8-sig')
        # ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
        return [(row['é–‹å§‹æ—¥'], row['çµ‚äº†æ—¥'], row['ã‚»ãƒ¼ãƒ«åˆ†é¡']) for _, row in df.iterrows()]
    except FileNotFoundError:
        st.error(f"âš ï¸ ã‚»ãƒ¼ãƒ«æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
        return []
    except Exception as e:
        st.error(f"âš ï¸ ã‚»ãƒ¼ãƒ«æƒ…å ±ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

SALE_PERIODS = load_sale_periods()

def classify_sale(target_date):
    """æ—¥ä»˜ã‹ã‚‰ã‚»ãƒ¼ãƒ«åˆ†é¡ã‚’åˆ¤å®š"""
    if pd.isna(target_date):
        return None

    if isinstance(target_date, str):
        target_date = pd.to_datetime(target_date).date()
    elif hasattr(target_date, 'date'):
        target_date = target_date.date()

    for start_str, end_str, sale_type in SALE_PERIODS:
        start = datetime.strptime(start_str, "%Y-%m-%d").date()
        end = datetime.strptime(end_str, "%Y-%m-%d").date()
        if start <= target_date <= end:
            return sale_type

    return None

st.set_page_config(page_title="Keepa Excelçµåˆãƒ„ãƒ¼ãƒ«", page_icon="ğŸ“Š", layout="wide")

st.title("ğŸ“Š Keepa Excelçµåˆãƒ„ãƒ¼ãƒ«")
st.markdown("è¤‡æ•°ã® `keepa-*.xlsx` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ASINåˆ—ã‚’è¿½åŠ ã—ã¦1ã¤ã®CSVã«çµåˆã—ã¾ã™ã€‚")

# ===== ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ– =====
if 'merged_df' not in st.session_state:
    st.session_state.merged_df = None
if 'file_list' not in st.session_state:
    st.session_state.file_list = []

# ===== ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ =====
st.subheader("ğŸ“¤ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")

uploaded_files = st.file_uploader(
    "keepa-*.xlsx ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰",
    type=["xlsx"],
    accept_multiple_files=True,
    help="Keepaå½¢å¼ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"
)

if uploaded_files:
    file_info = []
    for f in uploaded_files:
        # ã‚·ãƒ¼ãƒˆåã‹ã‚‰ASINã‚’å–å¾—ï¼ˆNoteä»¥å¤–ã®æœ€åˆã®ã‚·ãƒ¼ãƒˆï¼‰
        try:
            sheet_names = pd.ExcelFile(f).sheet_names
            asin = next((s for s in sheet_names if s.lower() != "note"), "ä¸æ˜")
        except Exception:
            asin = "ä¸æ˜"

        size_mb = len(f.getvalue()) / (1024 * 1024)

        file_info.append({
            "ãƒ•ã‚¡ã‚¤ãƒ«å": f.name,
            "ASIN": asin,
            "ã‚µã‚¤ã‚º (MB)": f"{size_mb:.2f}",
            "ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ": f
        })

    st.session_state.file_list = file_info
    st.success(f"âœ… {len(uploaded_files)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")

# ===== ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆè¡¨ç¤º =====
if st.session_state.file_list:
    st.subheader(f"ğŸ“‹ æ¤œå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ ({len(st.session_state.file_list)} ä»¶)")

    # è¡¨ç¤ºç”¨ã«ãƒ‘ã‚¹ã‚’é™¤å¤–
    display_df = pd.DataFrame(st.session_state.file_list)
    display_columns = ["ãƒ•ã‚¡ã‚¤ãƒ«å", "ASIN", "ã‚µã‚¤ã‚º (MB)"]
    st.dataframe(display_df[display_columns], use_container_width=True)

    # ===== çµåˆå‡¦ç† =====
    st.divider()

    if st.button("ğŸ”— çµåˆå®Ÿè¡Œ", type="primary", use_container_width=True):
        all_data = []
        progress_bar = st.progress(0)
        status_text = st.empty()

        for idx, file_info in enumerate(st.session_state.file_list):
            try:
                # ã‚·ãƒ¼ãƒˆåã‹ã‚‰æ­£ç¢ºãªASINã‚’å–å¾—
                status_text.text(f"å‡¦ç†ä¸­: {file_info['ãƒ•ã‚¡ã‚¤ãƒ«å']}")

                # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰ã®ã¿ï¼‰
                wb = openpyxl.load_workbook(
                    io.BytesIO(file_info["ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ"].getvalue()),
                    data_only=True
                )

                # ASINã‚·ãƒ¼ãƒˆã‚’æ¢ã™ï¼ˆNoteã‚·ãƒ¼ãƒˆä»¥å¤–ã®æœ€åˆã®ã‚·ãƒ¼ãƒˆ = ASINåï¼‰
                asin = next((name for name in wb.sheetnames if name.lower() != "note"), None)

                if not asin:
                    st.warning(f"âš ï¸ {file_info['ãƒ•ã‚¡ã‚¤ãƒ«å']}: ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆNoteã‚·ãƒ¼ãƒˆä»¥å¤–ï¼‰")
                    continue

                # ã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿
                file_info["ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ"].seek(0)
                df = pd.read_excel(file_info["ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ"], sheet_name=asin)

                # Aåˆ—ã«ASINã‚’è¿½åŠ ï¼ˆæ—¢å­˜ã‚«ãƒ©ãƒ ã‚’å³ã«ã‚·ãƒ•ãƒˆï¼‰
                df.insert(0, "ASIN", asin)

                # Båˆ—ï¼ˆæ—¥ä»˜ã‚«ãƒ©ãƒ ï¼‰ã®å­˜åœ¨ç¢ºèªã¨ã‚»ãƒ¼ãƒ«åˆ†é¡è¿½åŠ 
                date_col = None
                if "æ—¥ä»˜" in df.columns:
                    date_col = "æ—¥ä»˜"
                elif "Date" in df.columns:
                    date_col = "Date"

                if date_col:
                    # æ—¥ä»˜å‹ã«å¤‰æ›
                    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
                    # Cåˆ—ã«ã‚»ãƒ¼ãƒ«åˆ†é¡ã‚’è¿½åŠ 
                    df.insert(2, "ã‚»ãƒ¼ãƒ«åˆ†é¡", df[date_col].apply(classify_sale))

                # ã‚«ãƒ©ãƒ ã®æ•´ç†ã¨è¿½åŠ 
                # å®šä¾¡: FBAä¾¡æ ¼ã¨Listä¾¡æ ¼ã®æœ€å¤§å€¤
                fba_col = "FBA ä¾¡æ ¼(ï¿¥)" if "FBA ä¾¡æ ¼(ï¿¥)" in df.columns else "FBAä¾¡æ ¼(ï¿¥)"
                list_col = "List ä¾¡æ ¼(ï¿¥)" if "List ä¾¡æ ¼(ï¿¥)" in df.columns else "Listä¾¡æ ¼(ï¿¥)"

                if fba_col in df.columns and list_col in df.columns:
                    df["å®šä¾¡"] = df[[fba_col, list_col]].max(axis=1)
                elif fba_col in df.columns:
                    df["å®šä¾¡"] = df[fba_col]
                elif list_col in df.columns:
                    df["å®šä¾¡"] = df[list_col]

                # è²©å£²ä¾¡æ ¼: Buyboxä¾¡æ ¼
                buybox_col = "Buybox ä¾¡æ ¼(ï¿¥)" if "Buybox ä¾¡æ ¼(ï¿¥)" in df.columns else "Buyboxä¾¡æ ¼(ï¿¥)"
                if buybox_col in df.columns:
                    df["è²©å£²ä¾¡æ ¼"] = df[buybox_col]

                # ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼BSR: BSR[****]ç³»ã‚«ãƒ©ãƒ ã®æœ€å°å€¤
                bsr_columns = [col for col in df.columns if col.startswith("BSR[") and col.endswith("]")]
                if bsr_columns:
                    df["ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼BSR"] = df[bsr_columns].min(axis=1)

                # ä¸è¦ã‚«ãƒ©ãƒ ã®å‰Šé™¤
                cols_to_drop = [
                    "Buybox ä¾¡æ ¼(ï¿¥)", "Buyboxä¾¡æ ¼(ï¿¥)",
                    "ä¾¡æ ¼(ï¿¥)",
                    "Prime ä¾¡æ ¼(ï¿¥)", "Primeä¾¡æ ¼(ï¿¥)",
                    "Coupon ä¾¡æ ¼(ï¿¥)", "Couponä¾¡æ ¼(ï¿¥)",
                    "Coupon å‰²å¼•", "Couponå‰²å¼•",
                    "Deal ä¾¡æ ¼(ï¿¥)", "Dealä¾¡æ ¼(ï¿¥)",
                    "Deal ä¾¡æ ¼æƒ…å ±", "Dealä¾¡æ ¼æƒ…å ±",
                    "FBA ä¾¡æ ¼(ï¿¥)", "FBAä¾¡æ ¼(ï¿¥)",
                    "FBM ä¾¡æ ¼(ï¿¥)", "FBMä¾¡æ ¼(ï¿¥)",
                    "List ä¾¡æ ¼(ï¿¥)", "Listä¾¡æ ¼(ï¿¥)",
                    "è²©å£²æ•°(å­)",
                    "è©•ä¾¡", "è©•ä¾¡æ•°", "ã‚»ãƒ©ãƒ¼æ•°"
                ]
                # å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿å‰Šé™¤
                cols_to_drop_existing = [col for col in cols_to_drop if col in df.columns]
                df.drop(columns=cols_to_drop_existing, inplace=True)

                all_data.append(df)

            except Exception as e:
                st.error(f"âŒ {file_info['ãƒ•ã‚¡ã‚¤ãƒ«å']}: ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ - {str(e)}")
                continue

            # é€²æ—æ›´æ–°
            progress_bar.progress((idx + 1) / len(st.session_state.file_list))

        if all_data:
            # ç¸¦çµåˆï¼ˆsort=Falseã§BSRã‚«ãƒ©ãƒ ç­‰ã®å·®ç•°ã‚‚å…¨ã¦ä¿æŒï¼‰
            st.session_state.merged_df = pd.concat(all_data, ignore_index=True, sort=False)
            status_text.text("âœ… çµåˆå®Œäº†!")
            progress_bar.empty()
            st.success(f"ğŸ‰ çµåˆå®Œäº†: {len(all_data)} ãƒ•ã‚¡ã‚¤ãƒ« â†’ {len(st.session_state.merged_df)} è¡Œ")
        else:
            status_text.text("âŒ çµåˆã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            progress_bar.empty()

# ===== çµåˆçµæœè¡¨ç¤º =====
if st.session_state.merged_df is not None:
    st.divider()
    st.subheader("ğŸ“Š çµåˆçµæœ")

    # æ—¥ä»˜ã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèªã¨å‹å¤‰æ›
    date_column = None
    if "æ—¥ä»˜" in st.session_state.merged_df.columns:
        date_column = "æ—¥ä»˜"
        st.session_state.merged_df[date_column] = pd.to_datetime(
            st.session_state.merged_df[date_column], errors='coerce'
        )
    elif "Date" in st.session_state.merged_df.columns:
        date_column = "Date"
        st.session_state.merged_df[date_column] = pd.to_datetime(
            st.session_state.merged_df[date_column], errors='coerce'
        )

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ç·è¡Œæ•°", f"{len(st.session_state.merged_df):,}")
    with col2:
        unique_asins = st.session_state.merged_df["ASIN"].nunique()
        st.metric("ASINæ•°", unique_asins)
    with col3:
        st.metric("ã‚«ãƒ©ãƒ æ•°", len(st.session_state.merged_df.columns))

    # æ—¥ä»˜ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    filtered_df = st.session_state.merged_df.copy()

    if date_column and st.session_state.merged_df[date_column].notna().any():
        st.divider()
        st.subheader("ğŸ“… æ—¥ä»˜ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")

        min_date = st.session_state.merged_df[date_column].min().date()
        max_date = st.session_state.merged_df[date_column].max().date()

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–‹å§‹æ—¥: 12ãƒ¶æœˆå‰ã®æœˆåˆ
        today = datetime.now().date()
        default_start = (today.replace(day=1) - relativedelta(months=12))
        # ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²å†…ã«åã‚ã‚‹
        default_start = max(default_start, min_date)

        col_date1, col_date2 = st.columns(2)
        with col_date1:
            start_date = st.date_input(
                "é–‹å§‹æ—¥",
                value=default_start,
                min_value=min_date,
                max_value=max_date,
                help="ã“ã®æ—¥ä»˜ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 12ãƒ¶æœˆå‰ã®æœˆåˆï¼‰"
            )
        with col_date2:
            end_date = st.date_input(
                "çµ‚äº†æ—¥",
                value=max_date,
                min_value=min_date,
                max_value=max_date,
                help="ã“ã®æ—¥ä»˜ä»¥å‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"
            )

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
        if start_date <= end_date:
            mask = (
                (st.session_state.merged_df[date_column].dt.date >= start_date) &
                (st.session_state.merged_df[date_column].dt.date <= end_date)
            )
            filtered_df = st.session_state.merged_df[mask].copy()

            if len(filtered_df) < len(st.session_state.merged_df):
                st.info(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœ: {len(filtered_df):,} è¡Œ / {len(st.session_state.merged_df):,} è¡Œ")
        else:
            st.error("âš ï¸ é–‹å§‹æ—¥ã¯çµ‚äº†æ—¥ã‚ˆã‚Šå‰ã«è¨­å®šã—ã¦ãã ã•ã„")

    # ã‚µãƒãƒªãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
    st.divider()
    st.subheader("ğŸ“ˆ ã‚µãƒãƒªãƒ‡ãƒ¼ã‚¿")

    summary_data = []

    if "ã‚»ãƒ¼ãƒ«åˆ†é¡" in filtered_df.columns and "å®šä¾¡" in filtered_df.columns and "è²©å£²ä¾¡æ ¼" in filtered_df.columns:
        # ASIN Ã— ã‚»ãƒ¼ãƒ«ç¨®åˆ¥ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        for asin in filtered_df["ASIN"].unique():
            asin_df = filtered_df[filtered_df["ASIN"] == asin]

            # ç›´è¿‘ã®ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼BSRï¼ˆã‚»ãƒ¼ãƒ«é–¢ä¿‚ãªãæœ€æ–°æ—¥ä»˜ï¼‰
            latest_subcategory_bsr = None
            if "ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼BSR" in asin_df.columns:
                latest_row = asin_df.sort_values(date_column, ascending=False).iloc[0]
                latest_subcategory_bsr = latest_row["ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼BSR"]

            # ã‚»ãƒ¼ãƒ«ç¨®åˆ¥ã”ã¨ã®é›†è¨ˆ
            for sale_type in ["MDE", "ãƒ“ãƒƒã‚°ã‚»ãƒ¼ãƒ«", "ãƒ“ãƒƒã‚°ã‚»ãƒ¼ãƒ«ã®ã‚¢ãƒ¼ãƒªãƒ¼"]:
                sale_df = asin_df[asin_df["ã‚»ãƒ¼ãƒ«åˆ†é¡"] == sale_type].copy()

                if len(sale_df) > 0:
                    # å‚åŠ åˆ¤å®š: å®šä¾¡ã‹ã‚‰5%ä»¥ä¸Šå€¤ä¸‹ã’ã—ãŸæ—¥
                    sale_df["å€¤ä¸‹ã’ç‡"] = (sale_df["å®šä¾¡"] - sale_df["è²©å£²ä¾¡æ ¼"]) / sale_df["å®šä¾¡"]
                    participated_df = sale_df[sale_df["å€¤ä¸‹ã’ç‡"] >= 0.05]

                    # ã‚»ãƒ¼ãƒ«æœŸé–“å†…ã®ç·æ—¥æ•°ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç¯„å›²å†…ï¼‰
                    total_days = len(sale_df)

                    # å®Ÿéš›ã«å‚åŠ ã—ãŸæ—¥æ•°
                    participated_days = len(participated_df)

                    # å‚åŠ é »åº¦ï¼ˆ%ï¼‰
                    participation_rate = (participated_days / total_days * 100) if total_days > 0 else 0

                    # å®šä¾¡ï¼ˆå‚åŠ æ—¥ã®æœ€é »å€¤ã¾ãŸã¯å¹³å‡ï¼‰
                    list_price = None
                    if len(participated_df) > 0:
                        list_price = participated_df["å®šä¾¡"].mode()[0] if not participated_df["å®šä¾¡"].mode().empty else participated_df["å®šä¾¡"].mean()
                    else:
                        list_price = sale_df["å®šä¾¡"].mode()[0] if not sale_df["å®šä¾¡"].mode().empty else sale_df["å®šä¾¡"].mean()

                    # æœ€å®‰å€¤ãƒ»æœ€é«˜å€¤ã‚»ãƒ¼ãƒ«å£²ä¾¡ï¼ˆå‚åŠ æ—¥ã®ã¿ï¼‰
                    min_price = participated_df["è²©å£²ä¾¡æ ¼"].min() if len(participated_df) > 0 else None
                    max_price = participated_df["è²©å£²ä¾¡æ ¼"].max() if len(participated_df) > 0 else None

                    summary_data.append({
                        "ASIN": asin,
                        "å‚åŠ ã‚»ãƒ¼ãƒ«ç¨®åˆ¥": sale_type,
                        "ã‚«ãƒ†ã‚´ãƒªãƒ©ãƒ³ã‚¯ï¼ˆç›´è¿‘ï¼‰": latest_subcategory_bsr,
                        "å‚åŠ é »åº¦ï¼ˆ%ï¼‰": round(participation_rate, 1),
                        "å®šä¾¡": list_price,
                        "æœ€å®‰å€¤ã‚»ãƒ¼ãƒ«å£²ä¾¡": min_price,
                        "æœ€é«˜å€¤ã‚»ãƒ¼ãƒ«å£²ä¾¡": max_price
                    })

    summary_df = pd.DataFrame(summary_data)

    if not summary_df.empty:
        st.dataframe(summary_df, use_container_width=True)

        # ã‚µãƒãƒªCSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        summary_csv_buffer = io.StringIO()
        summary_df.to_csv(summary_csv_buffer, index=False, encoding="utf-8-sig")
        summary_csv_data = summary_csv_buffer.getvalue()

        st.download_button(
            label="ğŸ“Š ã‚µãƒãƒªCSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=summary_csv_data,
            file_name=f"keepa_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv",
            use_container_width=True
        )
    else:
        st.info("âš ï¸ ã‚µãƒãƒªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆã‚»ãƒ¼ãƒ«åˆ†é¡ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰")

    # è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    st.divider()
    st.subheader("ğŸ” è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")

    download_df = filtered_df.copy()

    # BSR[***]å½¢å¼ã®ã‚«ãƒ©ãƒ ã‚’æ¤œå‡º
    bsr_columns = [col for col in download_df.columns if col.startswith("BSR[") and col.endswith("]")]

    # å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã®æ‰€å±ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’åˆ¤å®šï¼ˆæœ€å°BSRå€¤ã‚’æŒã¤ã‚«ãƒ†ã‚´ãƒªãƒ¼ï¼‰
    if bsr_columns:
        def get_primary_category(row):
            """å„è¡Œã«ã¤ã„ã¦ã€æœ€å°BSRå€¤ã‚’æŒã¤ã‚«ãƒ†ã‚´ãƒªãƒ¼åã‚’è¿”ã™"""
            min_val = None
            min_category = None
            for col in bsr_columns:
                val = row[col]
                if pd.notna(val) and (min_val is None or val < min_val):
                    min_val = val
                    min_category = col[4:-1]  # "BSR[ã‚«ãƒ†ã‚´ãƒªãƒ¼å]" ã‹ã‚‰ ã‚«ãƒ†ã‚´ãƒªãƒ¼å ã‚’æŠ½å‡º
            return min_category

        download_df["ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼"] = download_df.apply(get_primary_category, axis=1)

        # ã‚«ãƒ†ã‚´ãƒªãƒ¼ä¸€è¦§ã‚’å–å¾—ï¼ˆNaNé™¤å¤–ï¼‰
        available_categories = sorted(download_df["ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼"].dropna().unique())

        if available_categories:
            col_cat, col_bsr = st.columns(2)

            with col_cat:
                selected_categories = st.multiselect(
                    "å±ã™ã‚‹ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼",
                    options=available_categories,
                    default=None,
                    help="è¤‡æ•°é¸æŠå¯èƒ½ï¼ˆORæ¡ä»¶ï¼‰ã€‚é¸æŠã—ãŸã‚«ãƒ†ã‚´ãƒªãƒ¼ã®ã„ãšã‚Œã‹ã«æ‰€å±ã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º"
                )

            with col_bsr:
                # ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼BSRç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                if "ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼BSR" in download_df.columns:
                    bsr_values = download_df["ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼BSR"].dropna()
                    if len(bsr_values) > 0:
                        min_bsr = int(bsr_values.min())
                        max_bsr = int(bsr_values.max())

                        bsr_range = st.slider(
                            "ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼BSRç¯„å›²",
                            min_value=min_bsr,
                            max_value=max_bsr,
                            value=(min_bsr, max_bsr),
                            help="ã“ã®ç¯„å›²å†…ã®BSRã‚’æŒã¤ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º"
                        )
                    else:
                        bsr_range = None
                else:
                    bsr_range = None

            # å¤§ã‚«ãƒ†ã‚´ãƒªBSRç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            main_bsr_range = None
            if "BSR" in download_df.columns:
                main_bsr_values = download_df["BSR"].dropna()
                if len(main_bsr_values) > 0:
                    main_bsr_min = int(main_bsr_values.min())
                    main_bsr_max = int(main_bsr_values.max())

                    main_bsr_range = st.slider(
                        "å¤§ã‚«ãƒ†ã‚´ãƒªBSRç¯„å›²",
                        min_value=main_bsr_min,
                        max_value=main_bsr_max,
                        value=(main_bsr_min, main_bsr_max),
                        help="ã“ã®ç¯„å›²å†…ã®å¤§ã‚«ãƒ†ã‚´ãƒªï¼ˆå…¨ä½“ï¼‰BSRã‚’æŒã¤ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º"
                    )

            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é©ç”¨
            filter_applied = False

            # ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if selected_categories:
                download_df = download_df[download_df["ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼"].isin(selected_categories)]
                filter_applied = True

            # ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼BSRç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if bsr_range and "ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼BSR" in download_df.columns:
                download_df = download_df[
                    (download_df["ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼BSR"] >= bsr_range[0]) &
                    (download_df["ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼BSR"] <= bsr_range[1])
                ]
                filter_applied = True

            # å¤§ã‚«ãƒ†ã‚´ãƒªBSRç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if main_bsr_range and "BSR" in download_df.columns:
                download_df = download_df[
                    (download_df["BSR"] >= main_bsr_range[0]) &
                    (download_df["BSR"] <= main_bsr_range[1])
                ]
                filter_applied = True

            if filter_applied:
                st.info(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœ: {len(download_df):,} è¡Œ / {len(filtered_df):,} è¡Œ")

    # è©³ç´°ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ©ãƒ ã‚’æ•´ç†ï¼ˆå¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿æ®‹ã™ï¼‰
    detail_columns = []
    if "ASIN" in download_df.columns:
        detail_columns.append("ASIN")
    if date_column:
        detail_columns.append(date_column)
    if "ã‚»ãƒ¼ãƒ«åˆ†é¡" in download_df.columns:
        detail_columns.append("ã‚»ãƒ¼ãƒ«åˆ†é¡")
    if "BSR" in download_df.columns:
        detail_columns.append("BSR")
    if "ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼" in download_df.columns:
        detail_columns.append("ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼")
    if "ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼BSR" in download_df.columns:
        detail_columns.append("ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼BSR")
    if "å®šä¾¡" in download_df.columns:
        detail_columns.append("å®šä¾¡")
    if "è²©å£²ä¾¡æ ¼" in download_df.columns:
        detail_columns.append("è²©å£²ä¾¡æ ¼")

    # å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    detail_columns = [col for col in detail_columns if col in download_df.columns]
    download_df = download_df[detail_columns]

    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    st.divider()
    st.markdown("**è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå…ˆé ­10è¡Œï¼‰**")

    # æ—¥ä»˜ã‚«ãƒ©ãƒ ã‚’æ—¥ä»˜ã®ã¿ã®è¡¨ç¤ºã«å¤‰æ›
    preview_df = download_df.head(10).copy()
    if date_column and date_column in preview_df.columns:
        preview_df[date_column] = preview_df[date_column].dt.strftime('%Y-%m-%d')

    st.dataframe(preview_df, use_container_width=True)

    # è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    st.divider()
    st.subheader("ğŸ’¾ è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"keepa_merged_{timestamp}.csv"

    csv_buffer = io.StringIO()
    download_df.to_csv(csv_buffer, index=False, encoding="utf-8-sig")
    csv_data = csv_buffer.getvalue()

    st.download_button(
        label="ğŸ’¾ è©³ç´°CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv_data,
        file_name=csv_filename,
        mime="text/csv",
        type="primary",
        use_container_width=True
    )

    st.info(f"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«å: `{csv_filename}` ({len(download_df):,} è¡Œ)")

# ===== ãƒ•ãƒƒã‚¿ãƒ¼ =====
st.divider()
st.caption("ğŸ“ Tips: è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€åº¦ã«é¸æŠã§ãã¾ã™ï¼ˆCtrl/Cmd + ã‚¯ãƒªãƒƒã‚¯ï¼‰ã€‚")
